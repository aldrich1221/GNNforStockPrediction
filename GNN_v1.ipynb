{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GNN_v1",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1IWpQbcdLAaSZaONVszYjPBRWvjvb6Xbr",
      "authorship_tag": "ABX9TyOLZ+zSC5MQ86GWZmke/kiN",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aldrich1221/GNNforStockPrediction/blob/main/GNN_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1aFbOT1cnMk",
        "outputId": "b19a5ea6-4833-4502-899e-db5fa8cfed52"
      },
      "source": [
        "!pip install quandl\n",
        "!pip install torch\n",
        "!pip install --no-index torch-scatter -f https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\n",
        "!pip install --no-index torch-sparse -f https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\n",
        "!pip install --no-index torch-cluster -f https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\n",
        "#!pip install --no-index torch-spline-conv -f https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\n",
        "!pip install torch-geometric\n",
        "!pip install networkx\n",
        "!pip install matplotlib\n",
        "!pip install numpy\n",
        "!pip install sklearn\n",
        "!pip install arch\n",
        "!pip install tushare\n",
        "!pip install pyflux\n",
        "!pip install sklearn\n",
        "!pip install apyori"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: quandl in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: pandas>=0.14 in /usr/local/lib/python3.6/dist-packages (from quandl) (1.1.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from quandl) (1.15.0)\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.6/dist-packages (from quandl) (2.23.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.6/dist-packages (from quandl) (8.6.0)\n",
            "Requirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.6/dist-packages (from quandl) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from quandl) (2.8.1)\n",
            "Requirement already satisfied: inflection>=0.3.1 in /usr/local/lib/python3.6/dist-packages (from quandl) (0.5.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.14->quandl) (2018.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->quandl) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->quandl) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->quandl) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->quandl) (1.24.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.7.0+cu101)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\n",
            "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.6/dist-packages (2.0.5)\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\n",
            "Requirement already satisfied: torch-sparse in /usr/local/lib/python3.6/dist-packages (0.6.8)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch-sparse) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy->torch-sparse) (1.19.5)\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\n",
            "Requirement already satisfied: torch-cluster in /usr/local/lib/python3.6/dist-packages (1.5.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y95l6470Nexj"
      },
      "source": [
        "Parameter Setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b8u_fntNdug"
      },
      "source": [
        "ClassNum=5\n",
        "ClassThreshold=[-0.01,-0.0025,0.0025,0.01]  #lenth must be ClassNum-1\n",
        "AssociationPara=[0.1,0.1,1.7,0.01]\n",
        "Corr=0.75\n",
        "fakeNode=True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82g14xx3NvbY"
      },
      "source": [
        "Packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLbY7P93NlnI"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq0PodpEcz9A"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import quandl, datetime\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch_scatter import scatter_add\n",
        "from torch_geometric.nn import MessagePassing\n",
        "import math\n",
        "\n",
        "StoragePath=\"/content/drive/MyDrive/MyCoLabStorage/GraphNeuralNetworkData\"\n",
        "\n",
        "API_Key=\"NJB8FEA7zxkhzwHsj5Z7\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwNdiEDudAFG"
      },
      "source": [
        "def assignClass(AllLableFrame,classNum,ClassThreshold):\n",
        "   import numpy as np\n",
        "    \n",
        "   for i in range(len(AllLableFrame)):\n",
        "      for j in range(len(AllLableFrame.columns)):\n",
        "        value=AllLableFrame.loc[i][j]\n",
        "        newValue=0\n",
        "        for iclass in range(classNum):\n",
        "          if iclass==0:\n",
        "\n",
        "            if value<=ClassThreshold[iclass]:\n",
        "              newValue=iclass\n",
        "          elif iclass==classNum-1:\n",
        "            if value>ClassThreshold[iclass-1]:\n",
        "              newValue=iclass\n",
        "          else:\n",
        "            if (value>ClassThreshold[iclass-1])&(value<=ClassThreshold[iclass]):\n",
        "              newValue=iclass   \n",
        "        AllLableFrame.loc[i][j]=newValue\n",
        "    \n",
        "   return AllLableFrame\n",
        "def columnsRename(data):\n",
        "  if 'Bid Average' in data.columns:\n",
        "    data.rename(columns={'Bid Average': 'Value'}, inplace=True)\n",
        "  if 'Close' in data.columns:\n",
        "    data.rename(columns={'Close': 'Value'}, inplace=True)\n",
        "  if 'Last' in data.columns:\n",
        "    data.rename(columns={'Last': 'Value'}, inplace=True)\n",
        "  if '5 YR' in data.columns:\n",
        "    data.rename(columns={'5 YR': 'Value'}, inplace=True)\n",
        "  if 'Short Volume' in data.columns:\n",
        "    data.rename(columns={'Short Volume': 'Value'}, inplace=True)\n",
        "  return data  \n",
        "\n",
        "APISymbolListFromYahoo=['OXY','CVX','XOM','COP','SLB','EOG','PSX','MPC','KMI','WMB','PXD','VER','ORA','FSLR','CEN','BLX']\n",
        "SymbolListFromYahoo=['OXY','CVX','XOM','COP','SLB','EOG','PSX','MPC','KMI','WMB','PXD','VER','ORA','FSLR','CEN','BLX']\n",
        "\n",
        "APISymbolListFromQuandl=['PERTH/GOLD_USD_D','PERTH/SLVR_USD_D','CHRIS/CME_CL1','CHRIS/CME_ES1','EURONEXT/RDSA','CHRIS/CME_S1','CHRIS/CME_C1','CHRIS/CME_W1']\n",
        "SymbolListFromQuandl=['GLD','Silver','Oil','S&P500','RDSA','SOYB','CORN','WHEAT']\n",
        "\n",
        "SymbolList=SymbolListFromYahoo+SymbolListFromQuandl\n",
        "APISymbolList=APISymbolListFromYahoo+APISymbolListFromQuandl\n",
        "\n",
        "\n",
        "AllDataFrame=pd.DataFrame()\n",
        "NoMADataFrame=pd.DataFrame()\n",
        "d=pd.DataFrame()\n",
        "columns=[]\n",
        "first=[]\n",
        "Nornalizerlist=[]\n",
        "for symbol in SymbolList:\n",
        "  dataFrame=pd.read_csv(StoragePath+\"/allStockRawData/\"+symbol+\".csv\")\n",
        "  \n",
        "  dataFrame=columnsRename(dataFrame)\n",
        "  columnPrice=symbol+\"_Value\"\n",
        "  # columnVolumn=symbol+\"_volume\"\n",
        "  columnFuturePrice=symbol+\"_ValueFuture\"\n",
        "  columns.append(columnFuturePrice)\n",
        "  check_for_nan_price = dataFrame['Value'].isnull()\n",
        "  #check_for_nan_volume = dataFrame['Adj. Close'].isnull()\n",
        "\n",
        "  dataFrame['Value'][check_for_nan_price]=np.median(dataFrame['Value'])\n",
        "  #dataFrame['Adj. Volume'][check_for_nan_volume]=np.median(dataFrame['Adj. Volume'])\n",
        "  A=dataFrame['Value'][~check_for_nan_price]\n",
        " \n",
        "  transformer = preprocessing.Normalizer().fit([A[:200]])\n",
        "\n",
        "  #Not do normalize\n",
        "  dataFrame['Value'][~check_for_nan_price]=transformer.transform([dataFrame['Value'][~check_for_nan_price]])[0]\n",
        "  \n",
        "  \n",
        "  Nornalizerlist.append(transformer)\n",
        "\n",
        "  NoMADataFrame[columnPrice]=dataFrame['Value']\n",
        "  AllDataFrame[columnPrice]=dataFrame['Value']\n",
        "  AllDataFrame[symbol+\"_MA5\"]=AllDataFrame[columnPrice].rolling(window=5).mean()\n",
        "\n",
        "\n",
        "  #AllDataFrame[columnVolumn]=dataFrame['Adj. Volume']\n",
        "  first.append(dataFrame['Value'][0])\n",
        "  \n",
        "  \n",
        "  d[columnFuturePrice]=dataFrame['Value'][1:]\n",
        "  \n",
        "\n",
        "LabelFrame=pd.DataFrame([first],columns=columns)\n",
        "\n",
        "LabelFrame=pd.concat([LabelFrame, d])\n",
        "\n",
        "AllLableFrame=(LabelFrame.diff()/LabelFrame)[6:]\n",
        "AllDailyReturnFrame = (AllDataFrame.diff() / AllDataFrame)[5:]\n",
        "AllLableFrame=AllLableFrame.reset_index(drop=True)\n",
        "AllDailyReturnFrame=AllDailyReturnFrame.reset_index(drop=True)\n",
        "NoMADataFrame=NoMADataFrame.diff()/NoMADataFrame\n",
        "\n",
        "\n",
        "\n",
        "AllYFrame=AllLableFrame\n",
        "AllLableFrame=assignClass(AllLableFrame,ClassNum,ClassThreshold)\n",
        "\n",
        "print(AllLableFrame.head(4))\n",
        "print(AllDailyReturnFrame.head(5))\n",
        "print(len(AllDailyReturnFrame))\n",
        "# A=[]\n",
        "# Class=[]\n",
        "# for i in range(ClassNum):\n",
        "#   Class.append([])\n",
        "# for i in AllLableFrame.columns:\n",
        "#   count=AllLableFrame[i].value_counts()\n",
        "#   A.append(AllLableFrame[i].value_counts())\n",
        "#   for j in range(ClassNum):\n",
        "\n",
        "#     print(Class[j],\"count[j]\",count[j])\n",
        "#     Class[j].append(count[j])\n",
        "    \n",
        "# Total=np.sum(np.sum(Class))\n",
        "# print(np.sum(Class[0])/Total,np.sum(Class[1])/Total,np.sum(Class[2])/Total,np.sum(Class[3])/Total,np.sum(Class[4])/Total)\n",
        "# print(A)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rv0OCKTCBrQl"
      },
      "source": [
        "\n",
        "def AddGrachFeatures(Add,AllDailyReturnFrame):\n",
        "  if Add==False:\n",
        "    return AllDailyReturnFrame\n",
        "  \n",
        "  from scipy import  stats\n",
        "  import statsmodels.api as sm \n",
        "  import numpy as np\n",
        "  import pandas as pd\n",
        "  import matplotlib.pyplot as plt\n",
        "  import arch  \n",
        "  \n",
        "\n",
        "  fig = plt.figure(figsize=(20,5))\n",
        "  for isymbol in range(len(SymbolList)):\n",
        "    symbol=SymbolList[isymbol]\n",
        "   \n",
        "\n",
        "    \n",
        "    data=AllDailyReturnFrame[symbol+\"_Value\"][:100]\n",
        "    print(data)\n",
        "    import numpy as np\n",
        "    import pyflux as pf\n",
        "    import pandas as pd\n",
        "    \n",
        "    from datetime import datetime\n",
        "    import matplotlib.pyplot as plt\n",
        "    %matplotlib inline\n",
        "    model = pf.GARCH(np.array(data),p=1,q=1)\n",
        "    x = model.fit()\n",
        "    x.summary()\n",
        "    newFeatures=model.predict(h=len(AllDailyReturnFrame)-100).values\n",
        "    \n",
        "    Temp=np.zeros(100).tolist()\n",
        "    for eachfeature in newFeatures:\n",
        "      Temp.append(eachfeature[0])\n",
        "    AllDailyReturnFrame[symbol+\"_Garch\"]=Temp\n",
        "  AllDailyReturnFrame.to_csv(StoragePath+\"AllDailyReturnFrame.csv\")\n",
        "    \n",
        "  return AllDailyReturnFrame\n",
        "\n",
        "  \n",
        "def AddRandomFeature(Add,AllDailyReturnFrame):\n",
        "  if Add==False:\n",
        "    return AllDailyReturnFrame\n",
        "  for isymbol in range(len(SymbolList)):\n",
        "    symbol=SymbolList[isymbol]\n",
        "   \n",
        "    \n",
        "    \n",
        "    data=AllDailyReturnFrame[symbol+\"_Value\"]\n",
        "    NewFeature=[data[0] ]\n",
        "    for ieachdata in range(1,len(data)):\n",
        "      epsilon=np.random.rand(1)[0]*np.abs(data[ieachdata]-data[ieachdata-1])\n",
        "      NewFeature.append(np.random.randn(1)[0]*epsilon/2+data[ieachdata])\n",
        "    \n",
        "    AllDailyReturnFrame[symbol+\"_NewFeature\"]=NewFeature\n",
        "  return AllDailyReturnFrame\n",
        "#AllDailyReturnFrame=AddGrachFeatures(False,AllDailyReturnFrame)\n",
        "AllDailyReturnFrame=AddRandomFeature(True,AllDailyReturnFrame)\n",
        "print(AllDailyReturnFrame.head())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTtC_Pt8rIXF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6asMUliZZwqU"
      },
      "source": [
        "def ExploreMissingValue(trainData_0):\n",
        "    missing=pd.concat([trainData_0.isnull().any(),trainData_0.count()],axis=1)\n",
        "    column=['miss or not','count']\n",
        "    \n",
        "    missing=pd.DataFrame(list(missing.values),index=list(missing.index),columns=column)\n",
        "    max=missing['count'].max()\n",
        "    missing['missing count']=max-missing['count']\n",
        "    missing['missing ratio']=missing['missing count']/max\n",
        "    miss=missing[missing['count'] < max]\n",
        "    print(miss)\n",
        "    \n",
        "    return np.array(missing.index[missing['miss or not']==True])  ##return the index with missing value\n",
        "def FillNullWithMedian(DataFrame):\n",
        "  for col in DataFrame.columns:\n",
        "    NullIndex=np.where(np.array(DataFrame[col].isnull()))[0]\n",
        "    \n",
        "    Median=np.nanmedian(DataFrame[col])\n",
        "    \n",
        "    DataFrame[col][NullIndex]=Median\n",
        "\n",
        "FillNullWithMedian(AllDailyReturnFrame)   \n",
        "ExploreMissingValue(AllDailyReturnFrame)\n",
        "FillNullWithMedian(NoMADataFrame)\n",
        "fig , ax = plt.subplots()\n",
        "fig.subplots_adjust(hspace=0.1, wspace=0.1) \n",
        "ax2 = ax.twinx()\n",
        "\n",
        "\n",
        "for icol in range(int(len(AllDailyReturnFrame.columns)/2)):\n",
        "   #plt.subplot(len(AllDailyReturnFrame.columns)/2,len(AllDailyReturnFrame.columns)/2,icol+1)\n",
        "   plt.plot(AllDailyReturnFrame[AllDailyReturnFrame.columns[icol]])\n",
        "plt.show()\n",
        "\n",
        "AllDailyReturnFrame.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxod-QlX8XkL"
      },
      "source": [
        "\n",
        "def AssociationAnalysis(AllDailyReturnFrame_AssociationAnalysis,StartEdgeCorr,EndEdgeCorr,GoAssociationAnalysis):\n",
        "  if GoAssociationAnalysis==False:\n",
        "    return [],[]\n",
        "  from apyori import apriori\n",
        "  import pandas as pd\n",
        "  # from mlxtend.frequent_patterns import apriori\n",
        "  # from mlxtend.frequent_patterns import association_rules\n",
        "  AllData=[]\n",
        "  for date in AllDailyReturnFrame_AssociationAnalysis.index:\n",
        "    DateData=AllDailyReturnFrame_AssociationAnalysis.loc[date]\n",
        "    UpData=[]\n",
        "    NoChangeData=[]\n",
        "    DownData=[]\n",
        "    for eachSymbol in DateData.index:\n",
        "      if DateData[eachSymbol]>AssociationPara[3]:\n",
        "        \n",
        "        UpData.append(eachSymbol.split('_')[0])\n",
        "        \n",
        "      elif DateData[eachSymbol]<AssociationPara[3]:\n",
        "        DownData.append(eachSymbol.split('_')[0])\n",
        "      else:\n",
        "        NoChangeData.append(eachSymbol.split('_')[0])\n",
        "    \n",
        "    \n",
        "    AllData.append(UpData)\n",
        "    #AllData.append(NoChangeData)\n",
        "    AllData.append(DownData)\n",
        "    \n",
        "  \n",
        "  data = AllData\n",
        "\n",
        "\n",
        "  \n",
        "  association_rules = apriori(data, min_support=AssociationPara[0], min_confidence=AssociationPara[1], min_lift=AssociationPara[2], max_length=2) \n",
        "  association_results = list(association_rules)\n",
        "\n",
        " \n",
        "  for eachResult in association_results:\n",
        "    resultitems=list(eachResult.items)\n",
        "    item1Index=np.where(np.array(SymbolList)==resultitems[0])[0]\n",
        "    item2Index=np.where(np.array(SymbolList)==resultitems[1])[0]\n",
        "    print(\"Add\",item1Index,item2Index)\n",
        "    if len(item1Index)>0:\n",
        "      if len(item2Index)>0:\n",
        "        StartEdgeCorr.append(item1Index[0])\n",
        "        EndEdgeCorr.append(item2Index[0])\n",
        "    if len(item1Index)>0:\n",
        "      if len(item2Index)>0:\n",
        "        StartEdgeCorr.append(item2Index[0])\n",
        "        EndEdgeCorr.append(item1Index[0])\n",
        "    #print(product) # ex. RelationRecord(items=frozenset({'Basketball', 'Socks'}), support=0.25, ordered_statistics=[OrderedStatistic(items_base=frozenset({'Basketball'}), items_add=frozenset({'Socks'}), confidence=0.5, lift=2.0), OrderedStatistic(items_base=frozenset({'Socks'}), items_add=frozenset({'Basketball'}), confidence=1.0, lift=2.0)])\n",
        "    product=eachResult\n",
        "    pair = eachResult[0] \n",
        "    ##print(pair) ## ex. frozenset({'Basketball', 'Socks'})\n",
        "    products = [x for x in pair]\n",
        "    print(products) # ex. ['Basketball', 'Socks']\n",
        "    print(\"Rule: \" + products[0] + \" →→   \" + products[1])\n",
        "    print(\"Support: \" + str(product[1]))\n",
        "    print(\"Lift: \" + str(product[2][0][3]))\n",
        "   \n",
        "\n",
        "  print(len(EndEdgeCorr))\n",
        " \n",
        "  return StartEdgeCorr,EndEdgeCorr\n",
        "\n",
        "\n",
        "CorrDataFrame=NoMADataFrame.loc[:200].corr(method ='pearson')\n",
        "print(CorrDataFrame)\n",
        "# print(np.where(np.array(CorrDataFrame)>0.7)[0])\n",
        "\n",
        "StartEdgeCorr=np.where(np.array(CorrDataFrame)>Corr )[0]\n",
        "EndEdgeCorr=np.where(np.array(CorrDataFrame)>Corr)[1]\n",
        "print(StartEdgeCorr)\n",
        "StartEdgeFromApriori=[]\n",
        "EndEdgeFromApriori=[]\n",
        "\n",
        "for i in range(len(StartEdgeCorr)):\n",
        "  if StartEdgeCorr[i]!=EndEdgeCorr[i]:\n",
        "    StartEdgeFromApriori.append(StartEdgeCorr[i])\n",
        "    EndEdgeFromApriori.append(EndEdgeCorr[i])\n",
        "\n",
        "print(\"Cor:\",StartEdgeFromApriori)\n",
        "print(\"cor:\",EndEdgeFromApriori)\n",
        "\n",
        "\n",
        "\n",
        "StartEdgeFromApriori,EndEdgeFromApriori=AssociationAnalysis(NoMADataFrame.loc[:100],StartEdgeFromApriori,EndEdgeFromApriori,True)\n",
        "\n",
        "\n",
        "# StartEdgeFromApriori=[]\n",
        "# EndEdgeFromApriori=[]\n",
        "print(StartEdgeFromApriori)\n",
        "print(EndEdgeFromApriori)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fii99fOrAmkI"
      },
      "source": [
        "Model definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVMTuSeaAlf2"
      },
      "source": [
        "class MyGNN(MessagePassing):\n",
        "    def __init__(self, in_channels, out_channels,havabias):\n",
        "        super(MyGNN, self).__init__(aggr='add') \n",
        "        self.weight1=torch.nn.Parameter(torch.Tensor(in_channels, 64))\n",
        "        self.bias1 = torch.nn.Parameter(torch.Tensor(64))\n",
        "        self.weight2 = torch.nn.Parameter(torch.Tensor(64, out_channels))\n",
        "        self.bias2= torch.nn.Parameter(torch.Tensor(out_channels))\n",
        "        self.edge_weight = torch.ones((edge_index.size(1),),\n",
        "                                 dtype=x.dtype,\n",
        "                                 device=edge_index.device)  \n",
        "        if havabias:\n",
        "            self.bias = torch.nn.Parameter(torch.Tensor(out_channels))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "            \n",
        "\n",
        "        if self.weight1 is not None:\n",
        "          stdv = math.sqrt(3.0 / (self.weight1.size(-2) + self.weight1.size(-1)))\n",
        "          self.weight1.data.normal_(-stdv, stdv)\n",
        "        if self.weight2 is not None:\n",
        "          stdv = math.sqrt(3.0 / (self.weight2.size(-2) + self.weight2.size(-1)))\n",
        "          self.weight2.data.normal_(-stdv, stdv)\n",
        "        \n",
        "        if self.bias1 is not None:\n",
        "          self.bias1.data.fill_(0)\n",
        "        if self.bias2 is not None:\n",
        "          self.bias2.data.fill_(0)\n",
        "        if self.bias is not None:\n",
        "          self.bias1.data.fill_(0)\n",
        "          \n",
        "    def forward(self, x, edge_index):\n",
        "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
        "        \n",
        "        x = torch.matmul(x, self.weight1)+self.bias1  ##fully connected layer1\n",
        "        x=F.relu(x)\n",
        "        x = torch.matmul(x, self.weight2)+self.bias2  ##fully connected layer2\n",
        "       \n",
        "\n",
        "        row, col = edge_index\n",
        "        deg = degree(row, x.size(0), dtype=x.dtype)\n",
        "        \n",
        "        deg_inv_sqrt = deg.pow(-0.5)\n",
        "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
        "        return self.propagate(edge_index, size=(x.size(0), x.size(0)), x=x,\n",
        "                              norm=norm)\n",
        "    def message(self, x_j, norm):\n",
        "        return norm.view(-1, 1) * x_j\n",
        "   \n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, dataset):\n",
        "        \n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = MyGNN(dataset.num_node_features, 256,havabias=False)\n",
        "        self.conv2 = MyGNN(256, dataset.num_classes,havabias=True)\n",
        "        self.leakyrelu=torch.nn.LeakyReLU(0.01)\n",
        "        #self.conv1 = GATConv(dataset.num_node_features, 16)\n",
        "        #self.conv2 = GATConv(16, dataset.num_classes)\n",
        "        #self.lstm = nn.LSTM(256, 256, dropout=0.2)\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x= self.leakyrelu(x)\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z26ahuV9ucnM"
      },
      "source": [
        "\n",
        "from torch_geometric.datasets import Planetoid\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import MessagePassing,TopKPooling\n",
        "from torch_geometric.utils import add_self_loops, degree\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch_geometric.utils import remove_self_loops, add_self_loops\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import FastRGCNConv,GMMConv,GCN2Conv,GATConv\n",
        "import torch.nn as nn\n",
        "   \n",
        "\n",
        "def generateDataMethod2(X,Y,idx,NDays,RelationMatrix,ClassNum):\n",
        "    def Convert2GraphData(GraphNodeX,GraphNodeY,GraphStartEdge,GraphEndEdge,trainMask,testMask,ClassNum,GraphNodeText):\n",
        "      \n",
        "      x = torch.tensor(GraphNodeX, dtype=torch.float)\n",
        "      \n",
        "      y = torch.tensor(GraphNodeY, dtype=torch.long)\n",
        "     \n",
        "      edge_index = torch.tensor([GraphStartEdge,\n",
        "                            GraphEndEdge], dtype=torch.long)\n",
        "      \n",
        "      data = Data(x=x ,edge_index=edge_index, y=y,test_mask=testMask, train_mask=trainMask,num_classes=ClassNum,node_text=GraphNodeText)\n",
        "      return data\n",
        "    def Assign(value):\n",
        "          newValue=0\n",
        "          if value<=-0.03:\n",
        "            newValue=0\n",
        "          elif (value>-0.03)&(value<=-0.01):\n",
        "            newValue=1\n",
        "          elif (value>-0.01)&(value<=0.01):\n",
        "            newValue=2\n",
        "          elif (value>0.01)&(value<=0.03):\n",
        "            newValue=3\n",
        "          else:\n",
        "            newValue=4\n",
        "          return newValue\n",
        "    N=len(X)-1\n",
        "    \n",
        "    TrainIndexFrom=100+idx\n",
        "    TrainIndexTo=N-(NDays-idx)-1\n",
        "    TestIndexFrom=100+1+idx\n",
        "    TestIndexTo=N-(NDays-idx)\n",
        "\n",
        "\n",
        "    trainX=X.loc[np.array(range(TrainIndexFrom,TrainIndexTo)).tolist()].reset_index ()\n",
        "    testX=X.loc[np.array(range(TestIndexFrom,TestIndexTo)).tolist()].reset_index ()\n",
        "    \n",
        "    GraphNodeX_train=[]\n",
        "    GraphNodeX_test=[]\n",
        "    GraphNodeText=[]\n",
        "\n",
        "    GraphNodeY_train=np.array(Y.loc[TrainIndexTo].values).tolist()\n",
        "    GraphNodeY_test=np.array(Y.loc[TestIndexTo].values).tolist()\n",
        "    \n",
        "    GraphStartEdge=[]\n",
        "    GraphEndEdge=[]\n",
        "    \n",
        "    for isymbol in range(len(SymbolList)):\n",
        "      symbol=SymbolList[isymbol]\n",
        "      columnPrice=symbol+\"_Value\"\n",
        "      columnFeature1=symbol+\"_NewFeature\"\n",
        "      columnVolumn=symbol+\"_volume\"\n",
        "      columnFuturePrice=symbol+\"_ValueFuture\"\n",
        "      #combineTrainFeatures=trainX[columnPrice].tolist()+trainX[columnVolumn].tolist()\n",
        "      #combineTestFeatures=testX[columnPrice].tolist()+testX[columnVolumn].tolist()\n",
        "\n",
        "      combineTrainFeatures=trainX[columnPrice].tolist()\n",
        "      combineTestFeatures=testX[columnPrice].tolist()\n",
        "\n",
        "      GraphNodeX_train.append(combineTrainFeatures)\n",
        "      GraphNodeX_test.append(combineTestFeatures)\n",
        "      GraphNodeText.append(symbol)\n",
        "      \n",
        "      #GraphStartEdge,GraphEndEdge=generateEdge(GraphStartEdge,GraphEndEdge,isymbol,symbol)\n",
        "   \n",
        "    GraphStartEdge=StartEdgeFromApriori\n",
        "    GraphEndEdge=EndEdgeFromApriori\n",
        "    \n",
        "    if fakeNode==True:\n",
        "      for isymbol in range(len(SymbolList)):\n",
        "        symbol=SymbolList[isymbol]\n",
        "        columnPrice=symbol+\"_Value\"\n",
        "        columnFeature1=symbol+\"_MA5\"\n",
        "        columnVolumn=symbol+\"_volume\"\n",
        "        columnFuturePrice=symbol+\"_ValueFuture\"\n",
        "        #combineTrainFeatures=trainX[columnPrice].tolist()+trainX[columnVolumn].tolist()\n",
        "        #combineTestFeatures=testX[columnPrice].tolist()+testX[columnVolumn].tolist()\n",
        "\n",
        "        combineTrainFeatures=trainX[columnFeature1].tolist()\n",
        "        combineTestFeatures=testX[columnFeature1].tolist()\n",
        "\n",
        "        GraphNodeX_train.append(combineTrainFeatures)\n",
        "        GraphNodeX_test.append(combineTestFeatures)\n",
        "        GraphNodeText.append(symbol+\"_Fake\")\n",
        "\n",
        "\n",
        "        GraphStartEdge.append(isymbol)\n",
        "        GraphEndEdge.append(isymbol+len(SymbolList))\n",
        "        GraphStartEdge.append(isymbol+len(SymbolList))\n",
        "        GraphEndEdge.append(isymbol)\n",
        "\n",
        "\n",
        "        GraphNodeY_train.append(Assign(trainX[columnFeature1].tolist()[-1]))\n",
        "        GraphNodeY_test.append(Assign(testX[columnFeature1].tolist()[-1]))\n",
        "\n",
        "    trainData=Convert2GraphData(GraphNodeX_train,GraphNodeY_train,GraphStartEdge,GraphEndEdge,np.array(range(len(GraphNodeY_train))).tolist(),[],ClassNum,GraphNodeText)\n",
        "    testData =Convert2GraphData(GraphNodeX_test,GraphNodeY_test,GraphStartEdge,GraphEndEdge,[],np.array(range(len(GraphNodeY_test))).tolist(),ClassNum,GraphNodeText)\n",
        "    \n",
        "    return trainData,testData\n",
        "def plot_dataset(dataset):\n",
        "    import matplotlib.pyplot as plt\n",
        "    import networkx as nx\n",
        "    edges_raw = dataset.edge_index.numpy()\n",
        "    edges = [(x, y) for x, y in zip(edges_raw[0, :], edges_raw[1, :])]\n",
        "    labels = dataset.y.numpy()\n",
        "    labelsDict={}\n",
        "    for i in range(len(dataset.node_text)):\n",
        "      labelsDict[i]=dataset.node_text[i]\n",
        "    G = nx.Graph()\n",
        "   \n",
        "    G.add_nodes_from(list(range(np.max(edges_raw))))\n",
        "    G.add_edges_from(edges)\n",
        "    plt.subplot(111)\n",
        "    options = {\n",
        "                'node_size': 50,\n",
        "                'width': 0.6,\n",
        "    }\n",
        "    with_labels=True\n",
        "    if len(dataset.node_text)>20:\n",
        "      with_labels=False\n",
        "    values = labels\n",
        "\n",
        "    # actual code\n",
        "    colorLeft = np.array([112, 224, 112])\n",
        "    colorRight = np.array([224, 112, 112])\n",
        "    scaled = MinMaxScaler().fit_transform(values.reshape(-1, 1))\n",
        "    colors = np.array([a * colorRight + (1 - a) * colorLeft for a in scaled], dtype = np.int64)\n",
        "    nx.draw(G, with_labels=with_labels,labels=labelsDict, node_color=colors/255, cmap=plt.cm.tab10, font_weight='bold', **options)\n",
        "    \n",
        "    # fig.canvas.mpl_connect(\"motion_notify_event\", hover)\n",
        "\n",
        "    plt.show()\n",
        "    \n",
        "\n",
        "def test(data, train=True):\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    pred = model(data).max(dim=1)[1]\n",
        "   \n",
        "\n",
        "    if train:\n",
        "        correct += pred[:len(SymbolList)].eq(data.y[:len(SymbolList)]).sum().item()\n",
        "        return correct / (len(data.y[:len(SymbolList)]))\n",
        "    else:\n",
        "        \n",
        "        #print(\"pred: \",pred[data.test_mask],\"True: \",data.y[data.test_mask])\n",
        "        correct += pred[:len(SymbolList)].eq(data.y[:len(SymbolList)]).sum().item()\n",
        "        return correct / (len(data.y[:len(SymbolList)]))\n",
        "\n",
        "def targetAssetTest(data,targetIndex):\n",
        "  model.eval()\n",
        "\n",
        "  correct = 0\n",
        "  pred = model(data).max(dim=1)[1]\n",
        "\n",
        "  return pred[data.test_mask][targetIndex],data.y[data.test_mask][targetIndex]\n",
        "\n",
        "\n",
        "def train(data1,data2, plot=False):\n",
        "    train_accuracies, test_accuracies = list(), list()\n",
        "    for epoch in range(200):\n",
        "            \n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "            out = model(data1)\n",
        "            loss = F.nll_loss(out[data1.train_mask], data1.y[data1.train_mask])\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_acc = test(data1)\n",
        "            test_acc = test(data2, train=False)\n",
        "\n",
        "            train_accuracies.append(train_acc)\n",
        "            test_accuracies.append(test_acc)\n",
        "            print('Epoch: {:03d}, Loss: {:.5f}, Train Acc: {:.5f}, Test Acc: {:.5f}'.\n",
        "                  format(epoch, loss, train_acc, test_acc))\n",
        "\n",
        "    if plot:\n",
        "        plt.plot(train_accuracies, label=\"Train accuracy\")\n",
        "        plt.plot(test_accuracies, label=\"Validation accuracy\")\n",
        "        plt.xlabel(\"# Epoch\")\n",
        "        plt.ylabel(\"Accuracy\")\n",
        "        plt.legend(loc='upper right')\n",
        "        plt.show()\n",
        "    return train_accuracies,test_accuracies\n",
        "\n",
        "def generateEdge(GraphStartEdge,GraphEndEdge,ThisNodeIndex,symbol):\n",
        "    #Node that ,to draw graph,the edge must be a bidirectional link\n",
        "    print(\"generateEdge\",GraphStartEdge,GraphEndEdge,ThisNodeIndex,symbol)\n",
        "    if symbol=='GLD':   \n",
        "      GraphStartEdge.append(ThisNodeIndex+1) #GLD->Silver\n",
        "      \n",
        "      GraphEndEdge.append(ThisNodeIndex)\n",
        "      \n",
        "      GraphStartEdge.append(ThisNodeIndex) #Silver->GLD\n",
        "      \n",
        "      GraphEndEdge.append(ThisNodeIndex+1)\n",
        "      \n",
        "\n",
        "      GraphStartEdge.append(ThisNodeIndex+2) #GLD->Oil\n",
        "      GraphEndEdge.append(ThisNodeIndex)\n",
        "      GraphStartEdge.append(ThisNodeIndex) #Oil->GLD\n",
        "      GraphEndEdge.append(ThisNodeIndex+2)\n",
        "\n",
        "    elif symbol=='Silver':\n",
        "      GraphStartEdge.append(ThisNodeIndex+1) #Silver->Oil\n",
        "      GraphEndEdge.append(ThisNodeIndex)\n",
        "      GraphStartEdge.append(ThisNodeIndex) #Oil->Silver\n",
        "      GraphEndEdge.append(ThisNodeIndex+1)\n",
        "    elif symbol=='Oil':\n",
        "      GraphStartEdge.append(ThisNodeIndex) #Oil->RDSA\n",
        "      GraphEndEdge.append(ThisNodeIndex+3)\n",
        "      GraphStartEdge.append(ThisNodeIndex+3) #RDSA->Oil\n",
        "      GraphEndEdge.append(ThisNodeIndex)\n",
        "    elif symbol=='VIX':\n",
        "      GraphStartEdge.append(ThisNodeIndex) #VIX->DOWJOHN\n",
        "      \n",
        "      GraphEndEdge.append(ThisNodeIndex+1)\n",
        "      \n",
        "      GraphStartEdge.append(ThisNodeIndex+1) #DOWJOHN->VIX\n",
        "      \n",
        "      GraphEndEdge.append(ThisNodeIndex)\n",
        "      \n",
        "\n",
        "\n",
        "    return GraphStartEdge,GraphEndEdge\n",
        "   \n",
        "\n",
        "#generateDataMethod1\n",
        "def generateDataMethod1(X,Y,idx,NDays,RelationMatrix,ClassNum):\n",
        "    \n",
        "    def Convert2GraphData(GraphNodeX,GraphNodeY,GraphStartEdge,GraphEndEdge,trainMask,testMask,ClassNum,GraphNodeText):\n",
        "      x = torch.tensor(GraphNodeX, dtype=torch.float)\n",
        "      y = torch.tensor(GraphNodeY, dtype=torch.long)\n",
        "\n",
        "      edge_index = torch.tensor([GraphStartEdge,\n",
        "                            GraphEndEdge], dtype=torch.long)\n",
        "\n",
        "      data = Data(x=x ,edge_index=edge_index, y=y,test_mask=testMask, train_mask=trainMask,num_classes=ClassNum,node_text=GraphNodeText)\n",
        "      return data\n",
        "    \n",
        "    edgeStart=RelationMatrix[0][0]\n",
        "    edgeEnd=RelationMatrix[0][1]\n",
        "\n",
        "    N=len(X)-1\n",
        "    # trainX=X[idx:N-(NDays-idx)]\n",
        "    # trainY=Y.loc[idx:N-(NDays-idx)]\n",
        "    # testX=X[1+idx:N-(NDays-idx)+1]\n",
        "    # testY=Y.loc[N-(NDays-idx)+1]\n",
        "    \n",
        "    GraphNodeX=[]\n",
        "    GraphNodeY=[]\n",
        "    GraphStartEdge=[]\n",
        "    GraphEndEdge=[]\n",
        "    GraphNodeText=[]\n",
        "    \n",
        "    Test_Mask=[]\n",
        "    Discard_Mask=[]\n",
        "    Train_Mask=[]\n",
        "    for time in range(1,len(X)):\n",
        "    #for time in range(1,3):\n",
        "      if time>N-(NDays-idx) :     #The data we want to predict\n",
        "        ##Start Test Point\n",
        "        mask=True\n",
        "      else:\n",
        "        mask=False\n",
        "      if time>N-(NDays-idx)+1:   #The data comes from future\n",
        "        dis_mask=True\n",
        "      else:\n",
        "        dis_mask=False\n",
        "      \n",
        "      if time>=idx:\n",
        "        train_mask=True\n",
        "      else:\n",
        "        train_mask=False\n",
        "      \n",
        "      eachTimeStartIndex=len(GraphNodeY)\n",
        "      ## eachTimeStartIndex-len(SymbolList):GLD_t-1,eachTimeStartIndex+1-len(SymbolList):Silver_t-1 ....\n",
        "      #SymbolList=['GLD','Silver','Oil','VIX','DOWJOHN','RDSA']\n",
        "     \n",
        "      for isymbol in range(len(SymbolList)):\n",
        "        symbol=SymbolList[isymbol]\n",
        "        Test_Mask.append(mask)\n",
        "        Discard_Mask.append(dis_mask)\n",
        "        Train_Mask.append(train_mask)\n",
        "        ##Features in each Node\n",
        "        #nodeFeatures=[X.loc[time][symbol+\"_Value\"],X.loc[time-1][symbol+\"_Value\"],X.loc[time][symbol+\"_MA5\"],X.loc[time-1][symbol+\"_MA5\"]]\n",
        "        nodeFeatures=X.loc[time].tolist()+X.loc[time-1].tolist()\n",
        "        GraphNodeText.append(symbol+\"_time\"+str(time))\n",
        "        GraphNodeX.append(nodeFeatures)\n",
        "        GraphNodeY.append(Y.loc[time][symbol+\"_ValueFuture\"])\n",
        "        ThisNodeIndex=eachTimeStartIndex+isymbol\n",
        "        \n",
        "        GraphStartEdge,GraphEndEdge=generateEdge(GraphStartEdge,GraphEndEdge,ThisNodeIndex,symbol)\n",
        "       \n",
        "          \n",
        "\n",
        "    \n",
        "    testMask=list(set(np.where(np.array(Test_Mask)==True)[0] .tolist()).intersection(set(np.where(np.array(Discard_Mask)==False)[0].tolist())))\n",
        "    trainMask=list(set(np.where(np.array(Train_Mask)==True)[0] .tolist()).intersection(set(np.where(np.array(Test_Mask)==False)[0].tolist())))\n",
        "\n",
        "   \n",
        "   \n",
        "    return Convert2GraphData(GraphNodeX,GraphNodeY,GraphStartEdge,GraphEndEdge,trainMask,testMask,ClassNum,GraphNodeText)\n",
        "\n",
        "\n",
        "  \n",
        "def OverallGraph(Overall):\n",
        "    plt.hist(Overall, label=\"test accuracy\", bins = ClassNum)\n",
        "    \n",
        "    #plt.bar(Overall, label=\"test accuracy\")\n",
        "   \n",
        "    plt.xlabel(\"accuracy range\")\n",
        "    plt.ylabel(\"Day Count\")\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.show()\n",
        "def BuyHoldTrading(MethodDict,price):\n",
        "  AssetAmout=MethodDict['AssetStockAmout'][-1]\n",
        "  AssetValue=MethodDict['AssetValue'][-1]\n",
        "  MethodDict['AssetValue'].append(AssetValue)\n",
        "  MethodDict['AssetStockAmout'].append(AssetAmout)\n",
        "  MethodDict['Action'].append(0)\n",
        "  MethodDict['TotalAssetValue'].append(MethodDict['AssetStockAmout'][-1]*price+MethodDict['AssetValue'][-1])\n",
        "  return MethodDict\n",
        "def Trading(data,target,price,MethodDict):\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  correct = 0\n",
        "  pred = model(data).max(dim=1)[1]\n",
        "\n",
        "  action=pred[data.test_mask][target]\n",
        "  if action>np.floor(ClassNum/2):\n",
        "    AssetAmout=MethodDict['AssetStockAmout'][-1]+1\n",
        "\n",
        "    AssetValue=MethodDict['AssetValue'][-1]-price\n",
        "\n",
        "\n",
        "    MethodDict['AssetValue'].append(AssetValue)\n",
        "    MethodDict['AssetStockAmout'].append(AssetAmout)\n",
        "    MethodDict['Action'].append(action)\n",
        "    MethodDict['TotalAssetValue'].append(MethodDict['AssetStockAmout'][-1]*price+MethodDict['AssetValue'][-1])\n",
        "\n",
        "    return MethodDict\n",
        "  elif action<np.floor(ClassNum/2):\n",
        "    AssetAmout=MethodDict['AssetStockAmout'][-1]-1\n",
        "\n",
        "    AssetValue=MethodDict['AssetValue'][-1]+price\n",
        "\n",
        "\n",
        "    MethodDict['AssetValue'].append(AssetValue)\n",
        "    MethodDict['AssetStockAmout'].append(AssetAmout)\n",
        "    MethodDict['Action'].append(action)\n",
        "    MethodDict['TotalAssetValue'].append(MethodDict['AssetStockAmout'][-1]*price+MethodDict['AssetValue'][-1])\n",
        "    return MethodDict\n",
        "  else:\n",
        "    AssetAmout=MethodDict['AssetStockAmout'][-1]\n",
        "\n",
        "    AssetValue=MethodDict['AssetValue'][-1]\n",
        "\n",
        "\n",
        "    MethodDict['AssetValue'].append(AssetValue)\n",
        "    MethodDict['AssetStockAmout'].append(AssetAmout)\n",
        "    MethodDict['Action'].append(action)\n",
        "    MethodDict['TotalAssetValue'].append(MethodDict['AssetStockAmout'][-1]*price+MethodDict['AssetValue'][-1])\n",
        "    return MethodDict\n",
        "def UpdatePriceList(AllDailyReturnFrame,stockIndex,testDayIndex,Method,StockPriceList):\n",
        "  columnsName=AllDailyReturnFrame.columns[stockIndex*2]\n",
        "  if Method==1:\n",
        "    TestDayReturn=AllDailyReturnFrame.loc[testDayIndex][columnsName]\n",
        "  elif Method==2:\n",
        "    TestDayReturn=AllDailyReturnFrame.loc[testDayIndex-1][columnsName]\n",
        "  StockPrice=StockPriceList[-1]*(1+TestDayReturn)\n",
        "  StockPriceList.append(StockPrice)\n",
        "  return StockPriceList\n",
        "def ClearPosition(MethodDict,StockPriceList):\n",
        "\n",
        "  if MethodDict['AssetStockAmout'][-1]>0:\n",
        "    MethodDict['AssetValue'].append(MethodDict['AssetValue'][-1]+StockPriceList[-1]*MethodDict['AssetStockAmout'][-1])\n",
        "    MethodDict['AssetStockAmout'].append(0)\n",
        "    MethodDict['Action'].append(-1)\n",
        "    MethodDict['TotalAssetValue'].append(MethodDict['AssetValue'][-1]+StockPriceList[-1]*MethodDict['AssetStockAmout'][-1])\n",
        "  elif MethodDict['AssetStockAmout'][-1]<0:\n",
        "    MethodDict['AssetValue'].append(MethodDict['AssetValue'][-1]-StockPriceList[-1]*MethodDict['AssetStockAmout'][-1])\n",
        "    MethodDict['AssetStockAmout'].append(0)\n",
        "    MethodDict['Action'].append(1)\n",
        "    MethodDict['TotalAssetValue'].append(MethodDict['AssetValue'][-1]-StockPriceList[-1]*MethodDict['AssetStockAmout'][-1])\n",
        "  else:\n",
        "    MethodDict['AssetValue'].append(MethodDict['AssetValue'][-1])\n",
        "    MethodDict['AssetStockAmout'].append(0)\n",
        "    MethodDict['Action'].append(0)\n",
        "    MethodDict['TotalAssetValue'].append(MethodDict['TotalAssetValue'][-1])\n",
        "\n",
        "  return MethodDict\n",
        "\n",
        "def OpenPosition(MethodDict,StockPriceList):\n",
        "  MethodDict['AssetValue'].append(0)\n",
        "  MethodDict['AssetStockAmout'].append(10)\n",
        "  MethodDict['Action'].append(1)\n",
        "  MethodDict['TotalAssetValue'].append(100)\n",
        "  return MethodDict\n",
        "\n",
        "def TradingMethodEvaluate(testDay,AllDailyReturnFrame,StockPriceList,BuyAndHoldDict,GNNDict,ModelData):\n",
        "  testDayIndex=len(AllDailyReturnFrame)-(TotalTestDay-testDay)+1\n",
        "  if testDayIndex==len(AllDailyReturnFrame)-(TotalTestDay)+1: #FirstDay\n",
        "    BuyAndHoldDict=OpenPosition(BuyAndHoldDict,StockPriceList)\n",
        "    GNNDict=OpenPosition(GNNDict,StockPriceList)\n",
        "    \n",
        "    StockPriceList=UpdatePriceList(AllDailyReturnFrame,stockIndex,testDayIndex,Method,StockPriceList)\n",
        "    GNNDict=Trading(ModelData,stockIndex,StockPriceList[-1],GNNDict)\n",
        "    \n",
        "  elif testDayIndex==len(AllDailyReturnFrame)-1:\n",
        "    StockPriceList=UpdatePriceList(AllDailyReturnFrame,stockIndex,testDayIndex,Method,StockPriceList)\n",
        "    GNNDict=ClearPosition(GNNDict,StockPriceList)\n",
        "    BuyAndHoldDict=ClearPosition(BuyAndHoldDict,StockPriceList)\n",
        "  else:\n",
        "    StockPriceList=UpdatePriceList(AllDailyReturnFrame,stockIndex,testDayIndex,Method,StockPriceList)\n",
        "\n",
        "    GNNDict=Trading(ModelData,stockIndex,StockPriceList[-1],GNNDict)\n",
        "    BuyAndHoldDict=BuyHoldTrading(BuyAndHoldDict,StockPriceList[-1])\n",
        "  return StockPriceList,BuyAndHoldDict,GNNDict\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEmNVvdqeoLg"
      },
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    import torch\n",
        "    from torch_geometric.data import Data\n",
        "\n",
        "    from torch_geometric.datasets import Planetoid\n",
        "  \n",
        "    PositiveRelation=[[0,1,0,2],\n",
        "                 [1,0,2,0]]\n",
        "\n",
        "    NegativeRelation=[[0,1,0,2],\n",
        "                 [1,0,2,0]]\n",
        "    relationMatrix=[PositiveRelation,NegativeRelation]\n",
        "    TotalTestDay=100\n",
        "    alloverAccuracy=[]\n",
        "    dowJohnArray=[]\n",
        "    predicY=[]\n",
        "    TrueY=[]\n",
        "    #trainData,testData=generateData(AllDailyReturnFrame,AllLableFrame,testDay,TotalTestDay,relationMatrix,ClassNum)\n",
        "    \n",
        "    Data1List=[]\n",
        "    Data2List=[]\n",
        "    targetAssetCorrect=[]\n",
        "    Method=2\n",
        "    print(\"preparing Graph Data.....using Method \",Method)\n",
        "\n",
        "    InitialAssetValue=100\n",
        "    InitialAssetAmout=100\n",
        "    AssetValue=InitialAssetValue;\n",
        "    AssetAmout=InitialAssetAmout;\n",
        "    StockPrice=10;\n",
        "    stockIndex=3\n",
        "    \n",
        "    StockPriceList=[10]\n",
        "\n",
        "    BuyAndHoldDict={\n",
        "        'AssetValue':[100],\n",
        "        'AssetStockAmout':[0],\n",
        "        'Action':[0],\n",
        "        'TotalAssetValue':[100]\n",
        "    }\n",
        "\n",
        "    GNNDict={\n",
        "        'AssetValue':[100],\n",
        "        'AssetStockAmout':[0],\n",
        "         'Action':[0],\n",
        "        'TotalAssetValue':[100]\n",
        "    }\n",
        "    \n",
        "    print(len(AllDailyReturnFrame))\n",
        "    \n",
        "    for testDay in range(TotalTestDay):\n",
        "      if Method==1:\n",
        "        alldata=generateDataMethod1(AllDailyReturnFrame,AllLableFrame,testDay,TotalTestDay,relationMatrix,ClassNum)\n",
        "        \n",
        "        Data1List.append(alldata)\n",
        "        Data2List.append(alldata)\n",
        "        print(\"Test Day\",testDay,\"Data\",Data1List[testDay],\"Test Node Index:\" ,Data1List[testDay].test_mask)\n",
        "      else: \n",
        "        traindata,testdata=generateDataMethod2(AllDailyReturnFrame,AllLableFrame,testDay,TotalTestDay,relationMatrix,ClassNum)\n",
        "        Data1List.append(traindata)\n",
        "        Data2List.append(testdata)\n",
        "        # print(\"Test Day\",testDay,\"Data\",Data1List[testDay],\"Predict Y :\" ,Data1List[testDay].y)\n",
        "        print(\"Test Day\",testDay,\"Data\",Data2List[testDay],\"Predict Y:\" ,Data2List[testDay].y)\n",
        "    \n",
        "    print(\"Begin to train and evaluate model...\")\n",
        "\n",
        "\n",
        "    ModelList=[]\n",
        "\n",
        "    train_accuracies_list=[]\n",
        "    test_accuracies_list=[]\n",
        "    for testDay in range(TotalTestDay):\n",
        "      Data1=Data1List[testDay]\n",
        "      Data2=Data2List[testDay]\n",
        "\n",
        "      # dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
        "      \n",
        "      #plot_dataset(Data1)\n",
        "    \n",
        "      device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "      model = Net(Data1).to(device)\n",
        "      data1 = Data1.to(device)\n",
        "      data2=Data2.to(device)\n",
        "\n",
        "      optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "      \n",
        "      train_accuracies,test_accuracies=train(data1,data2, plot=False)\n",
        "      \n",
        "      train_accuracies_list.append(train_accuracies)\n",
        "      test_accuracies_list.append(test_accuracies)\n",
        "      pred,groundtruth=targetAssetTest(data2,stockIndex)\n",
        "      targetAssetCorrect.append(pred==groundtruth)\n",
        "      ModelList.append(data2)\n",
        "      StockPriceList,BuyAndHoldDict,GNNDict=TradingMethodEvaluate(testDay,AllDailyReturnFrame,StockPriceList,BuyAndHoldDict,GNNDict,data2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    BH_return=(BuyAndHoldDict['AssetValue'][-1]-InitialAssetValue)/InitialAssetValue\n",
        "    GNN_return=(GNNDict['AssetValue'][-1]-InitialAssetValue)/InitialAssetValue\n",
        "    print(\"B&H\",BH_return,\"GNN\",GNN_return)\n",
        "    \n",
        "    print(\"=============\")\n",
        "    print(\"target Asset Accuracy: \",np.sum(targetAssetCorrect)/len(targetAssetCorrect))\n",
        "#       6\n",
        "# [1 0 3 6 3 0]\n",
        "# [(1, 0), (0, 1), (2, 0), (0, 2), (1, 0), (0, 1), (0, 3), (3, 0), (0, 1), (1, 0)]\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djUMMY4KKoRN"
      },
      "source": [
        "##overfitting?\n",
        "plt.subplot(2,1,1)\n",
        "plt.plot(np.mean(train_accuracies_list,axis=0),label='Training Accuracy')\n",
        "plt.plot(np.mean(test_accuracies_list,axis=0),label='Testing Accuracy')\n",
        "plt.legend(loc = 'upper right',fontsize=8,ncol=1)\n",
        "print(np.mean(np.mean(test_accuracies_list,axis=0)))\n",
        "plt.subplot(2,1,2)\n",
        "plt.plot(np.mean(test_accuracies_list,axis=0),label='Testing Accuracy')\n",
        "plt.legend(loc = 'upper right',fontsize=8,ncol=1)\n",
        "plt.xlabel('Epoch')\n",
        "plt.savefig('./trainingAccuracy.png')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QjgxqawAdrK"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    import torch\n",
        "    from torch_geometric.data import Data\n",
        "\n",
        "    from torch_geometric.datasets import Planetoid\n",
        "  \n",
        "    PositiveRelation=[[0,1,0,2],\n",
        "                 [1,0,2,0]]\n",
        "\n",
        "    NegativeRelation=[[0,1,0,2],\n",
        "                 [1,0,2,0]]\n",
        "    relationMatrix=[PositiveRelation,NegativeRelation]\n",
        "    AllSymbolAccuracy=[]\n",
        "    AllSymbolGroundTruth=[]\n",
        "    AllSymbolPredict=[]\n",
        "    for stockIndex in range(len(SymbolList)):\n",
        "    #for stockIndex in [1]:\n",
        "        SymbolGroudTruth=[]\n",
        "        SymbolPrediction=[]\n",
        "        TotalTestDay=100\n",
        "        alloverAccuracy=[]\n",
        "        dowJohnArray=[]\n",
        "        predicY=[]\n",
        "        TrueY=[]\n",
        "        #trainData,testData=generateData(AllDailyReturnFrame,AllLableFrame,testDay,TotalTestDay,relationMatrix,ClassNum)\n",
        "        \n",
        "        Method=2\n",
        "        print(\"preparing Graph Data.....using Method \",Method)\n",
        "\n",
        "        InitialAssetValue=100\n",
        "        InitialAssetAmout=100\n",
        "        AssetValue=InitialAssetValue;\n",
        "        AssetAmout=InitialAssetAmout;\n",
        "        StockPrice=10;\n",
        "        # stockIndex=12\n",
        "        targetAssetCorrect=[]\n",
        "        StockPriceList=[10]\n",
        "        print(\"Predict Asset: \",SymbolList[stockIndex])\n",
        "        BuyAndHoldDict={\n",
        "            'AssetValue':[100],\n",
        "            'AssetStockAmout':[0],\n",
        "            'Action':[0],\n",
        "            'TotalAssetValue':[100]\n",
        "        }\n",
        "\n",
        "        GNNDict={\n",
        "            'AssetValue':[100],\n",
        "            'AssetStockAmout':[0],\n",
        "            'Action':[0],\n",
        "            'TotalAssetValue':[100]\n",
        "        }\n",
        "        \n",
        "            \n",
        "        OverallAccuracyTrend=[]\n",
        "\n",
        "        for testDay in range(TotalTestDay-2):\n",
        "          \n",
        "            Data1=Data1List[testDay]\n",
        "            Data2=Data2List[testDay]\n",
        "\n",
        "            data2=ModelList[testDay]\n",
        "            pred,truth=targetAssetTest(data2,stockIndex)\n",
        "            targetAssetCorrect.append(pred==truth)\n",
        "            \n",
        "            SymbolGroudTruth.append(truth.item())\n",
        "            SymbolPrediction.append(pred.item())\n",
        "            OverallAccuracyTrend.append(test(data2, train=False))\n",
        "\n",
        "            StockPriceList,BuyAndHoldDict,GNNDict=TradingMethodEvaluate(testDay,AllDailyReturnFrame,StockPriceList,BuyAndHoldDict,GNNDict,data2)\n",
        "\n",
        "\n",
        "        AllSymbolGroundTruth.append(SymbolGroudTruth)\n",
        "        AllSymbolPredict.append(SymbolPrediction)\n",
        "\n",
        "\n",
        "        BH_return=(BuyAndHoldDict['AssetValue'][-1]-InitialAssetValue)/InitialAssetValue\n",
        "        GNN_return=(GNNDict['AssetValue'][-1]-InitialAssetValue)/InitialAssetValue\n",
        "        #print(\"B&H\",BH_return,\"GNN\",GNN_return)\n",
        "        \n",
        "        symbolAccuracy=np.sum(targetAssetCorrect)/len(targetAssetCorrect)\n",
        "        #print(\"=============\")\n",
        "        #print(\"target Asset Accuracy: \",np.sum(targetAssetCorrect)/len(targetAssetCorrect))\n",
        "        AllSymbolAccuracy.append(symbolAccuracy)\n",
        "\n",
        "    plt.plot(OverallAccuracyTrend)\n",
        "    plt.axhline(y=1/ClassNum, xmin=0, xmax=1,color='r')\n",
        "    plt.axhline(y=np.mean(OverallAccuracyTrend), xmin=0, xmax=1,color='g')\n",
        "len(AllSymbolGroundTruth[0])\n",
        "def F_score(AllSymbolGroundTruth,AllSymbolPredict):\n",
        "\n",
        "  from sklearn.metrics import f1_score\n",
        "  print(len(AllSymbolGroundTruth))\n",
        "  F_Micro=[]\n",
        "  F_Macro=[]\n",
        "  for isymbol in range(len(SymbolList)):\n",
        "\n",
        "    f1_micro = f1_score(AllSymbolGroundTruth[isymbol],AllSymbolPredict[isymbol],average='micro')\n",
        "    f1_macro = f1_score(AllSymbolGroundTruth[isymbol],AllSymbolPredict[isymbol],average='macro')\n",
        "\n",
        "    F_Micro.append(f1_micro)\n",
        "    F_Macro.append(f1_macro)\n",
        "  return F_Micro,F_Macro\n",
        "#       6\n",
        "# [1 0 3 6 3 0]\n",
        "# [(1, 0), (0, 1), (2, 0), (0, 2), (1, 0), (0, 1), (0, 3), (3, 0), (0, 1), (1, 0)]\n",
        "\n",
        "print(AssociationPara)\n",
        "\n",
        "print(\"Ave Acc: \",np.mean(AllSymbolAccuracy),\"Corr: \",Corr ,\"Assiciation: \",AssociationPara)\n",
        "print(AllSymbolAccuracy)\n",
        "\n",
        "F_Micro,F_Macro=F_score(AllSymbolGroundTruth,AllSymbolPredict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B71wMPFF4Sq3"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "fig, (ax1, ax2)= plt.subplots(2, 1, sharex='col')\n",
        "\n",
        "\n",
        "ax1.bar(SymbolList,AllSymbolAccuracy,label=\"Accuracy\",color='#cc6666')\n",
        "\n",
        "ax1.set_xticks(SymbolList, minor=False)\n",
        "ax1.set_xticklabels(SymbolList, rotation=90)\n",
        "ax1.axhline(y=np.mean(AllSymbolAccuracy), xmin=0, xmax=1,color='#FF6666',label='Average accuracy', linestyle='-.')\n",
        "# ax1.axhline(y=1/ClassNum, xmin=0, xmax=1,color='black',label='baseline')\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.subplots_adjust(wspace =0.1, hspace =0.1)#调整子图间距\n",
        "\n",
        "ax2.bar(SymbolList,F_Micro,label=\"F Micro\")\n",
        "ax2.axhline(y=np.mean(F_Micro), xmin=0, xmax=1,color='#0099FF',label='Average F Micro', linestyle='-.')\n",
        "print(np.mean(F_Macro))\n",
        "ax2.bar(SymbolList,F_Macro,label=\"F Macro\")\n",
        "ax2.axhline(y=np.mean(F_Macro), xmin=0, xmax=1,color='b',label='Average F Macro', linestyle='-.')\n",
        "# ax2.axhline(y=1/ClassNum, xmin=0, xmax=1,color='black')\n",
        "ax2.set_xticks(SymbolList, minor=False)\n",
        "ax2.set_xticklabels(SymbolList, rotation=90)\n",
        "lines = []\n",
        "labels = []\n",
        "\n",
        "for ax in fig.axes:\n",
        "    axLine, axLabel = ax.get_legend_handles_labels()\n",
        "    lines.extend(axLine)\n",
        "    labels.extend(axLabel)\n",
        "\n",
        "    \n",
        "fig.legend(lines, labels,           \n",
        "           loc = 'upper center',fontsize=8,ncol=3)\n",
        "fig.savefig('FScore.png')\n",
        "\n",
        "\n",
        "# data.plot.bar(ax=axes[1,1], color='b', alpha = 0.5)\n",
        "# data.plot.barh(ax=axes[0,1], color='k', alpha=0.5)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# plt.axhline(y=np.mean(AllSymbolAccuracy), xmin=0, xmax=1,color='g')\n",
        "# plt.axhline(y=1/ClassNum, xmin=0, xmax=1,color='black')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bw9NRUZcTlF"
      },
      "source": [
        "fig , ax = plt.subplots()\n",
        "fig.subplots_adjust(hspace=0.1, wspace=0.1) \n",
        "ax2 = ax.twinx()\n",
        "\n",
        "plt.subplot(4, 1, 1)\n",
        "plt.plot(StockPriceList)\n",
        "plt.subplot(4, 1, 2)\n",
        "plt.plot(GNNDict['Action'])\n",
        "plt.ylim(ymax=ClassNum-1)\n",
        "plt.axhline(y=np.floor(ClassNum/2), xmin=0, xmax=1,color='g')\n",
        "\n",
        "plt.subplot(4, 1, 3)\n",
        "plt.plot(GNNDict['TotalAssetValue'])\n",
        "plt.plot(BuyAndHoldDict['TotalAssetValue'])\n",
        "\n",
        "\n",
        "plt.subplot(4, 1, 4)\n",
        "\n",
        "plt.plot(GNNDict['AssetStockAmout'])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}